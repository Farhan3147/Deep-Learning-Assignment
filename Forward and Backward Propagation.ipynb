{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32714a23-30d9-47f8-851a-caf7c4d38496",
   "metadata": {},
   "source": [
    "# Forward and Backward Propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de9242-efff-492c-adfb-9436ce5617fd",
   "metadata": {},
   "source": [
    "# 1. Explain the concept of forward propagation in a neural network.\n",
    "\n",
    "Solution:-\n",
    "What is Forward Propagation?\n",
    "Forward propagation is the process in which an input passes through a neural network’s layers to produce an output. It is the first phase of neural network computation before backpropagation (which updates weights).\n",
    "Goal: Compute the predicted output (ŷ) based on input features (X) and the network’s weights & biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e18dc-7acf-4623-9376-01f936d9b050",
   "metadata": {},
   "source": [
    "# 2. What is the purpose of the activation function in forward propagation.\n",
    "\n",
    "An activation function introduces non-linearity in a neural network by transforming the weighted sum of inputs into an output signal. It helps the network learn complex patterns beyond simple linear relationships.\n",
    "\n",
    "Why Are Activation Functions Important in Forward Propagation?\n",
    "1️ Introduces Non-Linearity (Essential for Learning)\n",
    "Without activation functions, a neural network acts like a linear function, no matter how many layers it has.\n",
    "Non-linear activation functions allow the network to model complex relationships (e.g., recognizing faces, understanding speech)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff9953-c4bb-4e80-be52-22653939a837",
   "metadata": {},
   "source": [
    "# 3. Describe the steps involved in the backward propagation (backpropagation) algorithm.\n",
    "\n",
    "Solution:-\n",
    "Backpropagation (Backward Propagation of Errors) is an optimization algorithm used in neural networks to update the weights and biases by minimizing the error (loss). It works by calculating the gradient of the loss function with respect to each parameter using the chain rule of calculus.\n",
    "Steps of Backpropagation Algorithm\n",
    "Backpropagation occurs after forward propagation and consists of the following steps:\n",
    "1. Forward Propagation (Compute Output)\n",
    "2. Compute the Loss Gradient (Error Signal)\n",
    "3.  Backpropagate Error to Output Layer\n",
    "4.  Backpropagate Error to Hidden Layers\n",
    "5.  pdate Weights & Biases Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338550ac-0f71-468c-aeda-774696786361",
   "metadata": {},
   "source": [
    "# 4.  What is the purpose of the chain rule in backpropagation.\n",
    "\n",
    "Solution:-\n",
    "The chain rule is a fundamental concept from calculus that is used in backpropagation to compute the gradients of the loss function with respect to each weight in a neural network. It is critical for efficiently calculating how the error (loss) changes as the weights and biases are adjusted during training.\n",
    "\n",
    "The chain rule allows us to compute the derivative of a composite function. If you have a function that is composed of other functions, the chain rule helps to find the derivative of the entire function with respect to the variable of interest.\n",
    "\n",
    "In backpropagation, we need to compute how the loss function (error) changes with respect to each weight, starting from the output layer all the way to the input layer. This requires calculating gradients layer by layer, and the chain rule is the mathematical tool for that.\n",
    "\n",
    "Why is the Chain Rule Important in Backpropagation?\n",
    "Enables Efficient Gradient Computation\n",
    "The chain rule allows us to break down complex derivatives into smaller, manageable parts, making the process of computing gradients efficient.\n",
    "\n",
    "Backpropagates Error Through Layers\n",
    "By applying the chain rule layer by layer, backpropagation propagates the error backward from the output to the input layer, allowing each weight in the network to be updated correctly.\n",
    "\n",
    "Facilitates Optimization\n",
    "The gradients calculated using the chain rule guide the weight updates during training, helping the model converge toward a better solution (i.e., minimizing the loss function).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e316bd-20d0-4e4b-8b40-5556879b6355",
   "metadata": {},
   "source": [
    "# 5. Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\n",
    "\n",
    "Solution:-\n",
    "Here's a simple implementation of forward propagation for a neural network with one hidden layer using NumPy. We will define:\n",
    "\n",
    "An input layer with random values.\n",
    "A hidden layer with a ReLU activation function.\n",
    "An output layer with a sigmoid activation function (for binary classification).\n",
    "Steps:\n",
    "Input Layer: Random values (features).\n",
    "Hidden Layer: Compute weighted sum, apply ReLU activation.\n",
    "Output Layer: Compute weighted sum, apply Sigmoid activation (for probability output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98638d94-01da-4703-abae-e192192c1a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (Predicted Probability):\n",
      "[[0.33060082]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize parameters (weights and biases) for a neural network with 1 hidden layer\n",
    "input_size = 3  # Number of features (input layer size)\n",
    "hidden_size = 4  # Number of neurons in the hidden layer\n",
    "output_size = 1  # Output layer size (binary classification)\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights for input to hidden layer\n",
    "b1 = np.zeros((1, hidden_size))  # Biases for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size)  # Weights for hidden to output layer\n",
    "b2 = np.zeros((1, output_size))  # Biases for output layer\n",
    "\n",
    "# Sample input data (let's assume batch size = 1 for simplicity)\n",
    "X = np.array([[0.1, 0.2, 0.3]])  # Shape: (1, 3)\n",
    "\n",
    "# Forward Propagation\n",
    "# 1. Hidden Layer: Compute Z1, then apply ReLU activation\n",
    "Z1 = np.dot(X, W1) + b1  # Weighted sum for hidden layer\n",
    "A1 = np.maximum(0, Z1)  # ReLU activation\n",
    "\n",
    "# 2. Output Layer: Compute Z2, then apply Sigmoid activation\n",
    "Z2 = np.dot(A1, W2) + b2  # Weighted sum for output layer\n",
    "A2 = 1 / (1 + np.exp(-Z2))  # Sigmoid activation (output layer)\n",
    "\n",
    "# Output of forward propagation (A2 is the final predicted probability)\n",
    "print(\"Output (Predicted Probability):\")\n",
    "print(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377a3e6-8b31-48bd-a68d-1bd4fe06f5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
