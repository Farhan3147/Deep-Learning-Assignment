{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7aa70a-10bf-4c28-950f-9c9f08cc737d",
   "metadata": {},
   "source": [
    "# vggnet and resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba75ba-9a07-4dbd-ba3e-9a4f354ee0ad",
   "metadata": {},
   "source": [
    "# 1. Explain the architecture of VGGNet and ResNet. Compare and contrast their design principles and key components.\n",
    "\n",
    "Solution:-\n",
    "VGGNet Architecture\n",
    "VGGNet (Visual Geometry Group Network) is a deep convolutional neural network (CNN) that was introduced in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" by Simonyan and Zisserman in 2014. It is known for its simplicity and uniformity in design. The VGGNet architecture consists of the following key characteristics:\n",
    "\n",
    "Key Components of VGGNet:\n",
    "Convolutional Layers:\n",
    "\n",
    "VGGNet uses a series of small convolutional filters (3x3) stacked together, typically with a stride of 1 and padding to preserve spatial dimensions.\n",
    "The key principle behind VGGNet is the use of multiple layers of small convolution filters instead of larger filters, which allows the network to learn more complex features while maintaining computational efficiency.\n",
    "These small 3x3 filters can capture more fine-grained features as opposed to larger filters like 5x5 or 7x7.\n",
    "Pooling Layers:\n",
    "\n",
    "After a set of convolutional layers, max-pooling layers (2x2 with stride 2) are used for down-sampling. This reduces the spatial dimensions of the feature maps while keeping the important information intact.\n",
    "Fully Connected Layers:\n",
    "\n",
    "VGGNet ends with a few fully connected layers that help to classify the high-level features extracted by the convolutional and pooling layers. In the case of VGG16, there are three fully connected layers at the end, followed by a softmax layer for classification.\n",
    "Depth:\n",
    "\n",
    "VGGNet is known for its depth. The number in the name of the model (e.g., VGG16, VGG19) indicates the total number of layers in the network. VGG16 has 16 layers (13 convolutional and 3 fully connected layers).\n",
    "Design Principles of VGGNet:\n",
    "Small convolution filters (3x3): Instead of using large filters, VGGNet used small filters stacked on top of each other. A stack of two 3x3 convolutions has the same receptive field as a single 5x5 convolution, but it allows more non-linearity and a deeper architecture.\n",
    "Uniform architecture: VGGNet maintains a simple and uniform architecture where the number of filters increases as the network deepens (typically starting with 64 filters and doubling the number of filters after each block).\n",
    "Depth: The depth of the network allows it to learn hierarchical representations of features. VGGNet demonstrated that very deep architectures can lead to improved performance.\n",
    "ResNet Architecture\n",
    "ResNet (Residual Networks) was introduced in the paper \"Deep Residual Learning for Image Recognition\" by He et al. in 2015. ResNet is famous for its use of residual connections (also called skip connections), which allow the network to train very deep architectures effectively.\n",
    "\n",
    "Convolutional Layers:\n",
    "\n",
    "Like VGGNet, ResNet uses convolutional layers, but it introduces residual connections between them. These connections allow the network to avoid issues like vanishing gradients and enable much deeper networks.\n",
    "Pooling Layers:\n",
    "\n",
    "ResNet employs max-pooling layers to reduce the spatial resolution after the initial convolutions, similar to VGGNet.\n",
    "Fully Connected Layers:\n",
    "\n",
    "ResNet typically ends with fully connected layers for classification, though the depth and the structure can vary depending on the specific ResNet variant (e.g., ResNet-50, ResNet-101).\n",
    "Depth:\n",
    "\n",
    "ResNet allows for much deeper networks than VGGNet. For example, ResNet-50 has 50 layers, ResNet-101 has 101 layers, and ResNet-152 has 152 layers. The key innovation in ResNet is that these very deep networks can still be trained effectively due to the residual connections.\n",
    "Design Principles of ResNet:\n",
    "Residual connections (skip connections): These allow the network to learn identity mappings (i.e., the network can learn to skip certain layers if necessary), reducing the difficulty in training deeper networks. This is particularly useful for avoiding the vanishing gradient problem, which hinders the training of very deep networks.\n",
    "Depth: By using residual blocks, ResNet can be made much deeper than traditional CNNs like VGGNet without suffering from degradation problems, where adding more layers results in worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99dc05-782a-4ec1-b0c6-83e7240b5271",
   "metadata": {},
   "source": [
    "# 2. Discuss the motivation behind the residual connections in ResNet and the implications for training deep neural networks.\n",
    "\n",
    "Solution:-\n",
    "The primary motivation behind the introduction of residual connections (or skip connections) in ResNet was to address the challenges that arise when training very deep neural networks. Before ResNet, it was observed that simply stacking more layers in a neural network did not necessarily lead to better performance. Instead, deeper networks tended to degrade in performance, even when additional layers were added. This phenomenon is known as the degradation problem.\n",
    "\n",
    "Here are the key motivations behind introducing residual connections in ResNet:\n",
    "\n",
    "1. Degradation Problem in Deep Networks:\n",
    "Deeper Networks Perform Worse: As the depth of a neural network increases, its performance often starts to saturate and then degrade. This occurs despite the fact that deeper networks should, in theory, have the capacity to learn more complex and abstract features. The problem arises due to the difficulty of training deeper networks, caused by issues like vanishing gradients and exploding gradients.\n",
    "Training Difficulties: The gradients during backpropagation can either become exceedingly small (vanishing gradient problem) or very large (exploding gradient problem), making it difficult for the network to learn effectively.\n",
    "2. Vanishing Gradient Problem:\n",
    "In very deep networks, the gradients (which are used to update the weights during training) become smaller and smaller as they are propagated backward through the layers. This can cause weights in the earlier layers to stop updating, effectively preventing the model from learning from the data.\n",
    "Residual connections help mitigate this problem by creating shortcuts for the gradients to flow directly from one layer to another, bypassing intermediate layers and maintaining a more stable gradient flow.\n",
    "3. Learning Identity Functions:\n",
    "In very deep networks, layers may struggle to learn useful representations of the data, especially when the network is initialized randomly. The introduction of residual connections allows the network to learn identity functions, meaning that if adding a new layer doesn't improve the model's performance, the network can simply \"skip\" this layer by learning an identity mapping.\n",
    "This ability to learn identity mappings makes it easier for the network to perform well in practice, even with a large number of layers.\n",
    "Implications for Training Deep Neural Networks\n",
    "The introduction of residual connections in ResNet had several profound implications for the training of deep neural networks:\n",
    "\n",
    "1. Easier to Train Deeper Networks:\n",
    "With residual connections, deep networks can be trained effectively even with hundreds or thousands of layers. In traditional networks, adding more layers could result in a decrease in performance, but with ResNet, adding more layers can improve the network's ability to learn complex features, leading to better generalization.\n",
    "Example: ResNet-152 has 152 layers, and it performs better than networks with fewer layers like VGGNet (which has 16 or 19 layers) because of the residual connections that prevent degradation.\n",
    "2. Improved Gradient Flow:\n",
    "The key benefit of residual connections is that they allow gradients to flow more easily through the network during backpropagation. The shortcut connections ensure that the gradients are propagated directly through the residual mappings, reducing the chances of vanishing gradients.\n",
    "This results in faster convergence during training and allows for more stable optimization, especially in very deep networks.\n",
    "3. Increased Depth Without Degradation:\n",
    "One of the most significant outcomes of using residual connections is the ability to train networks with much greater depth. In traditional CNNs, adding more layers beyond a certain point leads to diminishing returns and can even hurt performance. With ResNet's residual connections, additional layers can be added without the risk of performance degradation.\n",
    "Implication: Researchers can now experiment with even deeper networks, leading to more powerful models that can capture more abstract representations of the data.\n",
    "4. Learning Shortcuts and Identity Mappings:\n",
    "The residual connections introduce the concept of learning identity mappings. Instead of forcing the network to learn complicated transformations at each layer, the network can choose to use residual connections when the transformation at a particular layer doesn't improve performance.\n",
    "Implication: This flexibility means that the network is more likely to converge to an optimal solution, as it can skip over unnecessary layers that do not contribute positively to the model's performance.\n",
    "5. Regularization and Robustness:\n",
    "The residual connections can also act as a form of regularization, making the network more robust. By allowing the layers to learn residual functions, rather than forcing them to learn transformations from scratch, the network is less likely to overfit to the training data.\n",
    "Implication: The network becomes more generalizable, leading to better performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b8d7b-9732-40d6-b9a1-3126f628b866",
   "metadata": {},
   "source": [
    "# 3. Examine the trade-offs between VGGNet and ResNet architectures in terms of computational complexity, memory requirements, and performance.\n",
    "\n",
    "Solution:-\n",
    "Both VGGNet and ResNet are highly influential deep learning architectures, and they represent different design philosophies. While VGGNet focuses on simplicity and uniformity, ResNet introduces residual connections to make very deep networks feasible. Below, we examine the trade-offs between VGGNet and ResNet in terms of computational complexity, memory requirements, and performance.\n",
    "\n",
    "1. Computational Complexity\n",
    "VGGNet:\n",
    "Increased Computational Cost with Depth:\n",
    "VGGNet is known for its simplicity and use of small 3x3 convolutional filters stacked on top of each other. This results in a high number of operations due to the deep architecture, especially in models like VGG16 and VGG19.\n",
    "Each layer in VGGNet performs convolutions and then feeds into the next layer, making the model computationally intensive, especially with large fully connected layers at the end.\n",
    "Example: VGG16 has approximately 138 million parameters (due to large fully connected layers), which leads to significant computational overhead.\n",
    "Inefficiency in Depth:\n",
    "Since there are no skip connections to facilitate the flow of information, VGGNet requires substantial computation at each layer without any shortcuts.\n",
    "ResNet:\n",
    "Reduced Computational Complexity (with skip connections):\n",
    "ResNet introduces residual (skip) connections, which allow deeper networks to be trained more effectively. These skip connections do not add computational overhead because they simply add the input of one layer to the output of another.\n",
    "However, in ResNet-50, ResNet-101, or other deeper variants, while the number of layers increases, the residual connections allow for more efficient training and backpropagation, which leads to more optimal computational performance in practice.\n",
    "Example: Despite having a larger number of layers (e.g., ResNet-152 with 152 layers), the residual connections ensure that the network can be trained effectively without needing as many operations as a similarly deep VGGNet would require.\n",
    "Key Trade-off:\n",
    "VGGNet is more computationally intensive due to its simple architecture and absence of residual connections. This results in high computational complexity, especially when considering its fully connected layers.\n",
    "ResNet is more computationally efficient at scale because of the residual connections, which reduce the need for excessive operations while enabling the effective use of very deep architectures.\n",
    "2. Memory Requirements\n",
    "VGGNet:\n",
    "High Memory Usage:\n",
    "VGGNet, especially the deeper versions like VGG16 or VGG19, have a large number of parameters, most of which are stored in the fully connected layers at the end.\n",
    "The fully connected layers (FC layers) are memory-intensive because they involve storing large weight matrices.\n",
    "Example: VGG16 contains around 138 million parameters. The majority of these parameters reside in the fully connected layers, making the model memory-heavy.\n",
    "ResNet:\n",
    "Lower Memory Usage (with residual connections):\n",
    "ResNet, despite being much deeper, typically has fewer parameters than VGGNet because it avoids large fully connected layers.\n",
    "The residual blocks use fewer parameters, as they only involve small convolutional layers and identity mappings. The use of skip connections doesn't require extra parameters for the connections themselves.\n",
    "Example: ResNet-50 has around 25 million parameters, which is significantly fewer than VGG16, despite being deeper and more complex.\n",
    "Key Trade-off:\n",
    "VGGNet has higher memory requirements due to the large number of parameters in the fully connected layers.\n",
    "ResNet has lower memory requirements due to its use of convolutional layers (as opposed to fully connected layers) and residual connections, which reduce the number of parameters needed.\n",
    "3. Performance (Accuracy and Generalization)\n",
    "VGGNet:\n",
    "Strong Performance on Smaller Networks:\n",
    "VGGNet performs very well for tasks that do not require ultra-deep networks or when a moderate level of complexity is sufficient (e.g., for less complex datasets or problems where depth is not crucial).\n",
    "VGG16 was highly effective in ImageNet classification at the time of its release and is known for its strong generalization capabilities when fine-tuned for specific tasks.\n",
    "Degradation with Depth:\n",
    "VGGNet's performance degrades as depth increases, and the lack of residual connections means that very deep versions of VGG (e.g., VGG19) struggle to improve accuracy compared to networks like ResNet.\n",
    "ResNet:\n",
    "Excellent Performance on Deeper Networks:\n",
    "The core advantage of ResNet lies in its ability to train very deep networks without performance degradation. It overcomes the vanishing gradient problem using residual connections, enabling models to be both very deep and highly accurate.\n",
    "ResNet-50 and ResNet-101 outperform many other networks, including VGGNet, on large-scale datasets like ImageNet, COCO, and Pascal VOC.\n",
    "ResNet also shows great generalization ability, allowing it to perform well even with very deep networks like ResNet-152.\n",
    "Key Trade-off:\n",
    "VGGNet is simpler and performs well with moderate depth but suffers from performance degradation in very deep networks due to its architecture.\n",
    "ResNet consistently achieves better accuracy and generalization across very deep networks due to the residual connections, enabling it to handle more complex tasks and large-scale problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670eaa86-e2e9-4760-94eb-bb3d5913bde7",
   "metadata": {},
   "source": [
    "# 4.  Explain how VGGNet and ResNet architectures have been adapted and applied in transfer learning scenarios. Discuss their effectiveness in fine-tuning pre-trained models on new tasks or datasets.\n",
    "\n",
    "Solution:-\n",
    "Transfer learning is a technique where a model that has been pre-trained on a large dataset (such as ImageNet) is adapted for use in a new, often smaller, dataset or task. In this context, both VGGNet and ResNet have been widely used due to their robust feature extraction capabilities. The process of fine-tuning pre-trained models involves adjusting the parameters of the model to make it better suited for the new task. Let's explore how VGGNet and ResNet are applied in transfer learning and their effectiveness in fine-tuning for new tasks or datasets.\n",
    "\n",
    "VGGNet in Transfer Learning\n",
    "Architecture Overview:\n",
    "VGGNet, with its simple architecture, consists of a series of convolutional layers followed by fully connected layers at the end. It is known for using small 3x3 convolution filters stacked in a deep configuration.\n",
    "\n",
    "Using VGGNet for Transfer Learning:\n",
    "Pre-training on Large Datasets (e.g., ImageNet):\n",
    "VGGNet is typically pre-trained on a large dataset such as ImageNet, where it learns to recognize a wide range of features in images. The deep convolutional layers extract hierarchical features like edges, textures, and object parts that are generally useful across various tasks.\n",
    "Adapting to New Tasks:\n",
    "Once pre-trained, the fully connected layers (which are specific to ImageNet categories) are often replaced or fine-tuned to suit the new task or dataset.\n",
    "The convolutional layers can be kept frozen (i.e., their weights are not updated during training) to preserve the learned feature extraction capabilities. This reduces computational cost and prevents overfitting, especially when working with small datasets.\n",
    "Fine-tuning Process:\n",
    "Layer Freezing and Replacing Fully Connected Layers:\n",
    "The typical approach is to freeze the weights of the initial convolutional layers and fine-tune the later layers, particularly the fully connected layers, for the new task.\n",
    "For example, in image classification, if you're applying VGGNet to a new dataset with different classes, you would replace the final fully connected layer (which outputs class predictions for ImageNet) with a new layer that outputs predictions for the new classes.\n",
    "Advantages of VGGNet in Transfer Learning:\n",
    "Simplicity: VGGNet's architecture is straightforward, making it easy to implement and understand for transfer learning tasks.\n",
    "Pre-learned Features: The deep convolutional layers capture generic features that can be effective for many computer vision tasks, even when the target dataset is quite different from ImageNet.\n",
    "Challenges with VGGNet:\n",
    "Memory and Computational Costs: VGGNet has a large number of parameters, particularly in the fully connected layers, which can make it less efficient for fine-tuning, especially on memory-constrained devices or when working with large datasets.\n",
    "Lack of Deep Residual Connections: The absence of residual connections can sometimes make training less effective compared to deeper architectures like ResNet.\n",
    "ResNet in Transfer Learning\n",
    "Architecture Overview:\n",
    "ResNet introduces the concept of residual connections that allow for very deep networks. These residual connections help address issues like the vanishing gradient problem, enabling much deeper networks to be effectively trained and used.\n",
    "\n",
    "Using ResNet for Transfer Learning:\n",
    "Pre-training on Large Datasets (e.g., ImageNet):\n",
    "Similar to VGGNet, ResNet is often pre-trained on large datasets like ImageNet. The model learns to extract features from a wide variety of objects. Thanks to residual connections, even deep versions of ResNet (like ResNet-50, ResNet-101, or ResNet-152) can be trained without degradation in performance.\n",
    "Adapting to New Tasks:\n",
    "The pre-trained convolutional layers in ResNet are highly effective in learning generic features that are transferable across tasks. Like VGGNet, the fully connected layers are typically replaced to suit the new dataset or task.\n",
    "The depth of ResNet, combined with the ability to learn more complex representations, makes it highly suitable for tasks involving large, diverse datasets.\n",
    "Fine-tuning Process:\n",
    "Layer Freezing and Fine-tuning:\n",
    "\n",
    "Similar to VGGNet, the typical process is to freeze the initial convolutional layers and fine-tune the later layers or the newly added fully connected layers. This allows the model to adapt to the new task without losing the useful feature extraction capabilities learned from the pre-training on ImageNet.\n",
    "Advantages of ResNet in Transfer Learning:\n",
    "\n",
    "Effective for Deep Networks: The introduction of residual connections makes ResNet highly effective in transfer learning scenarios, particularly when dealing with deeper networks. These networks are better at generalizing complex representations and can be more easily adapted to new tasks.\n",
    "Faster Convergence: Due to the residual connections, ResNet models generally converge faster and are easier to train than traditional deep networks, which is advantageous when performing transfer learning on new tasks.\n",
    "High Performance on Complex Tasks: ResNet models tend to achieve state-of-the-art performance on more challenging datasets and tasks due to their ability to handle very deep architectures.\n",
    "Challenges with ResNet:\n",
    "\n",
    "Complexity: While ResNet is more effective in training deeper networks, its architecture is more complex than VGGNet’s, making it harder to fine-tune, especially for beginners or in resource-constrained environments.\n",
    "Computational Cost: While residual connections help with training, ResNet models can still be computationally expensive, particularly the deeper variants (e.g., ResNet-101, ResNet-152). This can be a concern in transfer learning applications that involve limited hardware resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718cde3b-3213-48a2-aaf8-f16fba489f17",
   "metadata": {},
   "source": [
    "# 5.  Evaluate the performance of VGGNet and ResNet architectures on standard benchmark datasets such as ImageNet. Compare their accuracy, computational complexity, and memory requirements.\n",
    "\n",
    "Solution:-\n",
    "VGGNet and ResNet are both highly influential deep learning architectures that have demonstrated impressive results on standard benchmark datasets like ImageNet, which is a large-scale dataset used for image classification tasks. Below, we’ll evaluate and compare VGGNet and ResNet in terms of accuracy, computational complexity, and memory requirements on ImageNet.\n",
    "\n",
    "1. Accuracy Comparison on ImageNet\n",
    "VGGNet (VGG-16 and VGG-19):\n",
    "\n",
    "VGGNet-16 and VGGNet-19 are among the most widely used versions of VGGNet. Both networks are composed of 16 and 19 layers, respectively, with a sequence of convolutional layers followed by fully connected layers at the end.\n",
    "Top-5 Accuracy on ImageNet:\n",
    "VGG-16: Approximately 92.7% (Top-5 accuracy)\n",
    "VGG-19: Approximately 92.8% (Top-5 accuracy)\n",
    "Strength: VGGNet's accuracy on ImageNet is good, but it lags behind more modern architectures like ResNet due to its simplicity and lack of advanced techniques such as residual connections.\n",
    "ResNet (ResNet-50, ResNet-101, ResNet-152):\n",
    "\n",
    "ResNet-50 (with 50 layers), ResNet-101 (with 101 layers), and ResNet-152 (with 152 layers) all utilize residual connections to train very deep networks effectively.\n",
    "Top-5 Accuracy on ImageNet:\n",
    "ResNet-50: Approximately 93.3% (Top-5 accuracy)\n",
    "ResNet-101: Approximately 93.6% (Top-5 accuracy)\n",
    "ResNet-152: Approximately 94.1% (Top-5 accuracy)\n",
    "Strength: ResNet consistently outperforms VGGNet due to its deeper architecture and the use of residual connections, which help mitigate the vanishing gradient problem and allow the network to learn more complex features.\n",
    "Conclusion (Accuracy):\n",
    "ResNet achieves superior performance over VGGNet in terms of accuracy on ImageNet due to its deeper architecture, better feature learning, and residual connections.\n",
    "2. Computational Complexity\n",
    "VGGNet:\n",
    "\n",
    "VGGNet uses 3x3 convolutional filters stacked in deep layers. The computational complexity of VGGNet is high due to the large number of fully connected layers towards the end of the network. These layers contain a vast number of parameters (e.g., the fully connected layers of VGG-16 have around 138 million parameters).\n",
    "Computation Cost: High, due to the dense fully connected layers.\n",
    "Example: VGG-16 requires around 15.3 billion floating point operations (FLOPs) during inference.\n",
    "ResNet:\n",
    "\n",
    "ResNet reduces computational complexity by using residual connections, which allow the network to have deeper layers without exponentially increasing computational cost. ResNet also uses 1x1 convolutions to reduce the number of parameters in some layers, making it more efficient than traditional architectures like VGG.\n",
    "Computation Cost: Relatively lower than VGGNet for similar performance levels due to residual connections and efficient convolutional operations.\n",
    "Example: ResNet-50 requires approximately 4.1 billion FLOPs during inference, which is significantly lower than VGG-16's computation cost.\n",
    "Conclusion (Computational Complexity):\n",
    "ResNet is more computationally efficient compared to VGGNet, particularly as the depth increases. The use of residual connections and smaller convolutional filters in ResNet allows it to achieve higher accuracy with fewer floating point operations.\n",
    "3. Memory Requirements\n",
    "VGGNet:\n",
    "\n",
    "VGGNet's architecture, particularly with its fully connected layers, results in a very high memory footprint. The fully connected layers towards the end of the network have a large number of parameters that need to be stored, which makes VGGNet memory-hungry.\n",
    "Memory Usage: VGG-16 requires around 528 MB of memory for storing the model weights.\n",
    "ResNet:\n",
    "\n",
    "ResNet, while deeper, uses residual connections that allow it to be more memory efficient compared to VGGNet. The architecture is designed with a reduced number of parameters in certain layers (especially with 1x1 convolutions), which reduces the model size.\n",
    "Memory Usage: ResNet-50 requires around 98 MB of memory for storing the model weights, which is significantly lower than VGG-16.\n",
    "Conclusion (Memory Requirements):\n",
    "ResNet is more memory efficient than VGGNet, requiring less memory to store the model weights, which makes it more suitable for deployment on resource-constrained devices.\n",
    "4. Training Time and Convergence\n",
    "VGGNet:\n",
    "\n",
    "Training VGGNet is computationally expensive and slow due to the large number of parameters, especially in the fully connected layers. Its simplicity, however, makes it relatively easy to understand and implement.\n",
    "VGGNet may also require more epochs to converge, especially when fine-tuning on new tasks.\n",
    "ResNet:\n",
    "\n",
    "ResNet benefits from the use of residual connections, which facilitate faster convergence during training. The deep architecture combined with residuals allows the network to train more efficiently and overcome issues like vanishing gradients that might slow down convergence in deeper networks.\n",
    "ResNet models (especially ResNet-50 and ResNet-101) typically train faster and require fewer epochs to achieve comparable or better performance compared to VGGNet.\n",
    "Conclusion (Training Time and Convergence):\n",
    "ResNet converges faster than VGGNet due to its residual connections, allowing it to be more efficient in both training time and final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f35f1-6248-4374-b6fa-102e1d492d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
