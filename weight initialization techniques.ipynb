{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92227b1e-3c40-439c-a747-6073c2db7d1f",
   "metadata": {},
   "source": [
    "# weight initialization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb302b8-85d4-49dc-abad-733ff7dc1cf6",
   "metadata": {},
   "source": [
    "# 1. What is the vanishing gradient problem in deep neural networks? How does it affect training.\n",
    "\n",
    "Solution:-\n",
    "The vanishing gradient problem is a challenge encountered during the training of deep neural networks, particularly when using gradient-based optimization algorithms like backpropagation. It refers to the phenomenon where gradients (partial derivatives of the loss function with respect to model parameters) become exceedingly small as they are propagated backward through the layers of the network. This causes the weight updates in the earlier layers to be so small that they effectively stop learning, leading to poor performance and slow or stalled training.\n",
    "\n",
    "How It Affects Training:\n",
    "Slower Learning in Deep Networks:\n",
    "\n",
    "As gradients become smaller, the weight updates become negligible, especially for layers closer to the input. This results in a situation where the model's early layers hardly change, making it difficult for the network to learn meaningful features.\n",
    "The problem is more pronounced in networks with many layers, as the gradient diminishes exponentially with each successive layer.\n",
    "Difficulty in Training Deep Networks:\n",
    "\n",
    "In deep networks, the gradients are computed iteratively by applying the chain rule of calculus during backpropagation. In networks with many layers, the gradients can shrink exponentially as they propagate backward. If the gradients are very small, the updates to the weights are tiny, which makes training very slow and inefficient.\n",
    "Loss of Information:\n",
    "\n",
    "The vanishing gradient problem limits the network’s ability to learn from data in the early layers. The deeper the network, the more severe the vanishing gradient effect. This results in the network being unable to capture low-level features, as the signal is too weak to propagate effectively through the network.\n",
    "Causes of the Vanishing Gradient Problem:\n",
    "Activation Functions:\n",
    "Sigmoid and tanh activation functions are commonly associated with the vanishing gradient problem. These functions squash their input to a limited range (sigmoid: between 0 and 1, tanh: between -1 and 1). For extreme input values, the derivatives of these functions become very small, which leads to very small gradients during backpropagation.\n",
    "Weight Initialization:\n",
    "Poor weight initialization can also exacerbate the vanishing gradient problem. If the weights are initialized with very small values, the outputs and activations in the early layers can become very small, leading to small gradients.\n",
    "Solutions to the Vanishing Gradient Problem:\n",
    "Use of ReLU (Rectified Linear Unit) Activation Function:\n",
    "\n",
    "ReLU activation function, defined as f(x)=max(0,x), helps mitigate the vanishing gradient problem because its derivative is 1 for positive inputs, which prevents the gradients from becoming too small. It doesn’t saturate for large values of input, unlike sigmoid and tanh, where the gradients become very small for large positive or negative inputs.\n",
    "He and Xavier Initialization:\n",
    "\n",
    "He initialization (for ReLU) and Xavier initialization (for tanh and sigmoid) are techniques for initializing weights in a way that ensures proper scaling of gradients during backpropagation. These initialization methods aim to maintain a variance of activations and gradients that remains reasonable across layers.\n",
    "Batch Normalization:\n",
    "\n",
    "Batch normalization helps by normalizing the input to each layer, ensuring that activations stay within a certain range. This can reduce the likelihood of very large or very small gradients, which in turn alleviates the vanishing gradient problem.\n",
    "Residual Networks (ResNet):\n",
    "\n",
    "Residual connections (used in ResNet) allow the gradients to flow directly through the network without being affected by the depth of the network. These connections create \"shortcut paths\" that bypass one or more layers, allowing the gradients to propagate more effectively even in very deep networks.\n",
    "Gradient Clipping:\n",
    "\n",
    "Gradient clipping is a technique where gradients that exceed a certain threshold are scaled down to prevent extremely large gradients (exploding gradients), which are the opposite of vanishing gradients. Although this technique is more relevant to exploding gradients, it can help stabilize training in some cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a842cebf-9df2-466b-8b59-f0609196827b",
   "metadata": {},
   "source": [
    "# 2. Explain how Xavier initialization addresses the vanishing gradient problem.\n",
    "\n",
    "Solution:-\n",
    "Xavier initialization (also known as Glorot initialization) is a technique used to address the vanishing gradient problem by ensuring that the variance of activations and gradients is appropriately scaled across layers in a deep neural network. It plays a crucial role in making deep neural networks easier to train, especially when using activation functions like sigmoid and tanh, which are more susceptible to the vanishing gradient problem.\n",
    "\n",
    "How Xavier Initialization Works:\n",
    "Goal:\n",
    "\n",
    "The main goal of Xavier initialization is to maintain a constant variance of activations and gradients across layers. This helps avoid the problem where gradients either vanish (become too small) or explode (become too large) as they propagate through the network during training.\n",
    "The Problem in Deep Networks:\n",
    "\n",
    "When training a deep neural network, backpropagation requires the gradients to flow through each layer. If the weights are initialized poorly (either too small or too large), the activations and gradients can either vanish (become too small) or explode (become too large) as they propagate through the network. This can make the learning process slow or even cause it to fail entirely.\n",
    "\n",
    "Vanishing gradients are particularly problematic with activation functions like sigmoid and tanh, where the gradients become very small for large input values. This leads to weight updates becoming extremely small, which causes the network to stop learning effectively, especially in deeper layers.\n",
    "\n",
    "Xavier Initialization Approach:\n",
    "\n",
    "Xavier initialization works by setting the initial weights in a way that the variance of the activations is controlled, preventing them from becoming too small or too large as they pass through the network. The key idea is to set the initial weights so that the expected variance of the outputs of each layer matches the expected variance of the inputs.\n",
    "\n",
    "Specifically, the weights are drawn from a uniform distribution or a normal distribution with the following properties:\n",
    "\n",
    "This initialization ensures that the variance of the activations is roughly the same across layers, which prevents gradients from either vanishing or exploding.\n",
    "\n",
    "Impact on the Vanishing Gradient Problem:\n",
    "Balanced Gradients:\n",
    "\n",
    "Xavier initialization helps in keeping the gradients balanced. By scaling the weights according to the number of input neurons to a layer, it ensures that the variance of activations and gradients are consistent throughout the network.\n",
    "Prevents Vanishing and Exploding Gradients:\n",
    "\n",
    "By controlling the scale of the weights, Xavier initialization prevents the gradients from becoming too small (vanishing) or too large (exploding). In particular:\n",
    "When using activation functions like sigmoid or tanh, the gradients can easily vanish if the weights are initialized with values that are too small. Xavier initialization ensures that the weights are not too small, reducing the chance of vanishing gradients.\n",
    "It also prevents the weights from being too large, which would cause the activations to saturate and lead to exploding gradients.\n",
    "Improves Training Efficiency:\n",
    "\n",
    "By ensuring a consistent variance in the activations and gradients, Xavier initialization helps the network train faster and converge more efficiently, as it avoids issues caused by vanishing or exploding gradients.\n",
    "Xavier Initialization in Action:\n",
    "Before Xavier Initialization:\n",
    "In a deep network, if the weights are initialized too small, the activations and gradients shrink exponentially as they move through each layer, leading to vanishing gradients in the backpropagation process. This slows down or prevents learning.\n",
    "After Xavier Initialization:\n",
    "With Xavier initialization, the variance of activations is controlled, and as the gradients are propagated backward through the layers, they maintain a magnitude that allows for effective learning. This prevents the gradients from vanishing and ensures that the network can learn from both shallow and deep layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bfb8f-23b8-4bfd-9229-34b25006d551",
   "metadata": {},
   "source": [
    "# 3. What are some common activation functions that are prone to causing vanishing gradients.\f",
    "\n",
    "\n",
    "Solution:-\n",
    "Some common activation functions that are prone to causing vanishing gradients are:\n",
    "\n",
    "1. Sigmoid (Logistic) Activation Function:\n",
    "Problem: The vanishing gradient issue occurs because the sigmoid function squashes input values to a range between 0 and 1. For large positive or negative inputs, the derivative of the sigmoid becomes very small, nearing zero. This causes the gradient to vanish during backpropagation, especially for deep networks. The output values of the sigmoid also saturate at the extremes, leading to small gradients and slow learning.\n",
    "Effect on Training: For large or small input values, the gradient is extremely small, causing the weight updates to become negligible, which impedes learning in deep networks.\n",
    "2. Tanh (Hyperbolic Tangent) Activation Function:\n",
    "The tanh function is defined as:\n",
    "Problem: Like the sigmoid, the tanh function also squashes inputs, but to a range of [-1, 1]. The gradient of the tanh function approaches zero as the input values get larger or smaller. This means that during backpropagation, the gradients become very small when the activation is in the saturated regions of the function (near -1 or 1), resulting in vanishing gradients.\n",
    "Effect on Training: The saturation regions cause gradients to diminish as they are backpropagated through the network, making it difficult for the earlier layers to learn.\n",
    "3. Softmax (used in multi-class classification):\n",
    "The Softmax function is often used as an activation in the output layer for multi-class classification problems. It is defined as:\n",
    "Problem: Although the softmax function is not directly responsible for vanishing gradients, it can indirectly contribute to the problem in deeper networks. The gradients of softmax can become very small if the predicted values for one class are significantly higher than the others. In this case, the gradients of the other classes will be very small (close to zero), which can lead to slow or ineffective learning.\n",
    "Effect on Training: If the network is confident about the class predictions (i.e., a high value for one class), the gradients of the other classes will be close to zero, which can slow down the training process.\n",
    "Why These Activation Functions Lead to Vanishing Gradients:\n",
    "Saturation of Activation Functions:\n",
    "For functions like sigmoid and tanh, when the input is far from zero, the function becomes saturated. The slope of the function at these regions is very small, causing small derivatives and therefore small gradients during backpropagation.\n",
    "Exponential Growth in Backpropagation:\n",
    "When using gradient-based optimization, the gradients are propagated backward through the layers using the chain rule. In deep networks, this repeated multiplication of small gradients leads to a dramatic decrease in the size of gradients as they move backward, especially in the case of sigmoid and tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f35a4-e4f2-41d1-94d8-9febb6b48d99",
   "metadata": {},
   "source": [
    "# 4. Define the exploding gradient problem in deep neural networks. How does it impact training.\n",
    "\n",
    "Solution:-\n",
    "Exploding Gradient Problem in Deep Neural Networks\n",
    "The exploding gradient problem is the opposite of the vanishing gradient problem. It occurs when gradients become excessively large during backpropagation, particularly in deep neural networks. This problem arises when the gradients of the loss function grow exponentially as they are propagated backward through the network, making the training process unstable.\n",
    "\n",
    "How It Happens:\n",
    "Backpropagation involves computing gradients of the loss function with respect to the weights of the neural network. These gradients are used to update the weights of the network in the direction that minimizes the loss.\n",
    "If the gradients become very large during this process (especially when multiplying small numbers repeatedly through the layers), they can explode. The values of the gradients become extremely large, causing large updates to the weights.\n",
    "This is particularly common in very deep networks where the gradients are propagated back through many layers, and in the presence of certain activation functions or improper weight initialization.\n",
    "Impact of the Exploding Gradient Problem:\n",
    "Unstable Training:\n",
    "\n",
    "Large weight updates: As gradients become excessively large, they cause massive updates to the weights during training. This makes the training process unstable, and the model parameters may oscillate or even diverge instead of converging to a solution.\n",
    "Oscillation: Instead of steadily reducing the loss function, the model may start jumping erratically between weight values, unable to reach an optimal point.\n",
    "Loss of Convergence:\n",
    "\n",
    "Gradient explosion can cause the model to fail to converge at all, or it may result in the network being stuck in regions of the parameter space where the optimization cannot progress.\n",
    "In some cases, the loss function might increase dramatically due to the large updates, which will hinder the network from learning the correct patterns in the data.\n",
    "Numerical Instability:\n",
    "\n",
    "Very large gradients can lead to overflow or NaN (Not a Number) values in the training process, leading to numerical instability. This is because large numbers in computations might exceed the numerical limits of the system, causing computation errors.\n",
    "Ineffective Optimization:\n",
    "\n",
    "As a result of the large gradients and weight updates, optimization algorithms such as stochastic gradient descent (SGD) may perform inefficiently. The optimizer could fail to make progress in reducing the loss function, leading to poor performance.\n",
    "Causes of Exploding Gradients:\n",
    "Improper Weight Initialization:\n",
    "\n",
    "Initializing the weights of a network with very large values can cause large gradients during backpropagation, especially in deep networks. This is common in networks where weights are initialized with a large standard deviation or scale.\n",
    "Deep Networks:\n",
    "\n",
    "In deep networks, the chain rule of backpropagation involves multiplying gradients at each layer. If the weights or activations at each layer amplify the gradient, it can lead to exponential growth in gradients, especially in networks with many layers.\n",
    "Activation Functions:\n",
    "\n",
    "Some activation functions, such as the ReLU (Rectified Linear Unit) function, do not squash the output and can cause the gradients to grow large, especially if the network has many layers with large activations.\n",
    "Large Learning Rates:\n",
    "\n",
    "If the learning rate is set too high, even moderate gradients can cause drastic changes to the weights, resulting in divergence. When this is combined with exploding gradients, the updates become uncontrollable.\n",
    "Solutions to the Exploding Gradient Problem:\n",
    "Gradient Clipping:\n",
    "\n",
    "Gradient clipping involves capping the gradients at a specific threshold during backpropagation. If the gradients exceed a predefined limit, they are scaled down to prevent them from exploding.\n",
    "This technique is widely used to stabilize training in deep networks and especially recurrent neural networks (RNNs), where the exploding gradient problem is more common.\n",
    "Weight Initialization:\n",
    "\n",
    "Proper weight initialization can help avoid the problem of exploding gradients. Techniques like Xavier (Glorot) initialization or He initialization aim to scale the weights based on the number of neurons in the layer, preventing the gradients from becoming too large.\n",
    "Using smaller or well-scaled weight initialization methods can prevent large gradients from propagating back through the layers.\n",
    "Use of Activation Functions with Saturation Problems:\n",
    "\n",
    "Switching from ReLU to more bounded activation functions like Leaky ReLU or Sigmoid can help control large gradients. However, these alternatives may introduce their own set of problems (e.g., vanishing gradients), so careful consideration is needed.\n",
    "Adaptive Learning Rates:\n",
    "\n",
    "Optimizers like Adam, RMSProp, and Adagrad use adaptive learning rates, which adjust the step size based on the gradient magnitudes. These optimizers can help reduce the impact of large gradients and maintain more stable training.\n",
    "Batch Normalization:\n",
    "\n",
    "Batch normalization normalizes the activations within each mini-batch to ensure that the network operates on more stable and standardized data, reducing the likelihood of exploding gradients.\n",
    "Use of Residual Networks (ResNets):\n",
    "\n",
    "Residual connections or skip connections, as used in ResNet, allow gradients to flow directly through the network without being affected by deep layers. This helps prevent the gradients from either vanishing or exploding, especially in very deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb7d41-7c4a-4af8-8a80-cf2239564bf1",
   "metadata": {},
   "source": [
    "# 5. What is the role of proper weight initialization in training deep neural networks.\n",
    "\n",
    "Solution:-\n",
    "Proper weight initialization plays a crucial role in the training of deep neural networks. It refers to the practice of setting the initial values of the weights in the network before the training process begins. Good weight initialization helps ensure that the network can learn effectively and avoid problems such as vanishing gradients, exploding gradients, or slow convergence.\n",
    "\n",
    "Key Roles of Proper Weight Initialization:\n",
    "Avoiding Vanishing and Exploding Gradients:\n",
    "\n",
    "Vanishing gradients: If weights are initialized too small, the gradients during backpropagation can become exceedingly small, causing the learning process to slow down significantly or even stop in deep networks.\n",
    "Exploding gradients: Conversely, if weights are initialized too large, the gradients can become excessively large during backpropagation, leading to instability and causing the network to diverge.\n",
    "Proper weight initialization ensures that gradients are of a reasonable scale during backpropagation, helping to avoid these issues.\n",
    "Faster Convergence:\n",
    "\n",
    "Proper initialization can help speed up the convergence of the network during training. If the weights are initialized in a way that the network starts off in a favorable region of the loss landscape, the network can learn faster.\n",
    "Incorrect initialization can lead to longer training times, as the network may need to adjust the weights from an unfavorable starting point.\n",
    "Symmetry Breaking:\n",
    "\n",
    "If all the weights in a neural network are initialized to the same value (e.g., zero), all the neurons in a layer will behave the same way and learn the same features, which is highly problematic.\n",
    "Random initialization helps break symmetry, ensuring that different neurons in the network learn different features and contribute to the overall learning process. This is particularly important in layers with multiple neurons, such as the hidden layers of a neural network.\n",
    "Preventing Dead Neurons:\n",
    "\n",
    "If a layer's weights are initialized poorly, it can lead to dead neurons (neurons that never activate or contribute to the learning process). For example, if weights are too large or small, the neurons may always saturate, leading to a gradient of zero, and thus they stop learning altogether.\n",
    "Good initialization avoids these issues, ensuring neurons remain active and contribute to learning.\n",
    "Popular Weight Initialization Methods:\n",
    "Random Initialization:\n",
    "\n",
    "In this method, weights are initialized randomly, usually with a uniform or normal distribution. However, a basic random initialization may still cause vanishing or exploding gradients, so more advanced methods have been developed.\n",
    "Xavier (Glorot) Initialization:\n",
    "\n",
    "This method is designed to address the vanishing and exploding gradient problems for sigmoid or tanh activations. It initializes the weights by drawing from a distribution with variance inversely proportional to the number of inputs to the neuron.\n",
    "\n",
    "He Initialization:\n",
    "\n",
    "He initialization is an extension of Xavier initialization that is more suited for ReLU activations. It takes into account that ReLU activations are not symmetric around zero and are prone to the dying ReLU problem (where neurons become inactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1294d6-db6c-41f4-9172-f19f91abdcfd",
   "metadata": {},
   "source": [
    "# 6.  Explain the concept of batch normalization and its impact on weight initialization techniques.\n",
    "\n",
    "Solution:-\n",
    "Batch Normalization is a technique introduced to improve the training of deep neural networks by normalizing the input to each layer. The idea is to standardize the activations of each layer across mini-batches to ensure they have a consistent distribution during training. This helps in stabilizing and speeding up the training process, leading to better performance.\n",
    "\n",
    "How Batch Normalization Works:\n",
    "Normalization:\n",
    "\n",
    "For each mini-batch of data, the mean and variance of the activations are calculated for each feature (neuron) within the layer.\n",
    "The activations are then normalized by subtracting the mean and dividing by the standard deviation to ensure the activations have zero mean and unit variance.\n",
    "Learnable Parameters:\n",
    "\n",
    "After normalization, the outputs are scaled and shifted by two learnable parameters: gamma (γ) and beta (β). These parameters allow the network to learn the optimal scale and shift for each feature after normalization.\n",
    "\n",
    "During Training vs. Inference:\n",
    "\n",
    "During training, batch statistics (mean and variance) are calculated for each mini-batch.\n",
    "During inference (testing or deployment), the running averages of the mean and variance, computed during training, are used for normalization to ensure consistency.\n",
    "Impact of Batch Normalization on Weight Initialization Techniques:\n",
    "Batch Normalization helps address some of the challenges in training deep neural networks, particularly those related to weight initialization. Here's how it interacts with weight initialization:\n",
    "\n",
    "Reduces Dependence on Initialization:\n",
    "\n",
    "Without Batch Normalization: Poor weight initialization (e.g., using random small values) can cause problems such as vanishing or exploding gradients, and slow convergence. In deep networks, gradients can become too small or too large as they propagate back, making training inefficient.\n",
    "With Batch Normalization: BN reduces the effect of poor weight initialization by normalizing the inputs to each layer. This helps ensure that activations have consistent distributions and are less sensitive to how weights are initialized. It allows the model to train effectively even with less optimal weight initializations, reducing the need for careful initialization strategies like Xavier or He.\n",
    "Eases the Use of Larger Learning Rates:\n",
    "\n",
    "Batch Normalization stabilizes training by reducing the sensitivity of the network to the scale of activations. As a result, it allows the use of larger learning rates without the risk of instability (which is often the case when learning rates are too high with poorly initialized weights).\n",
    "Typically, networks without batch normalization may need to use smaller learning rates to avoid divergence, but BN allows the use of higher learning rates, speeding up convergence.\n",
    "Enables More Flexible Initialization:\n",
    "\n",
    "Without BN, you need to carefully choose weight initialization methods (e.g., Xavier or He) to avoid vanishing/exploding gradients. This requires careful tuning based on the activation functions used (e.g., ReLU, sigmoid, etc.).\n",
    "With BN, weight initialization becomes less critical. You can use a broader range of initialization strategies, as the normalization step in BN ensures the activations do not explode or vanish, making the network less sensitive to weight scaling.\n",
    "Improves the Optimization Landscape:\n",
    "\n",
    "Batch normalization transforms the optimization landscape by stabilizing the activations at each layer, making it easier for optimization algorithms (like gradient descent) to work more effectively.\n",
    "This reduces the likelihood of gradient issues (like vanishing or exploding gradients) that might otherwise arise from poorly initialized weights, allowing for more robust and consistent training.\n",
    "In Practice:\n",
    "With Batch Normalization, you can use simpler or more standard weight initialization methods without needing to worry too much about the specific activation function.\n",
    "Batch normalization allows larger weight values (compared to initializations with strict scaling based on the number of neurons) because the network has a built-in mechanism (normalization) that regulates the magnitude of activations and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e4da0-f822-4732-b6ae-c41063215884",
   "metadata": {},
   "source": [
    "# 7.  Implement He initialization in Python using TensorFlow or PyTorch.\n",
    "\n",
    "Solution:-\n",
    "Here’s how you can implement He initialization in both TensorFlow and PyTorch.\n",
    "\n",
    "1. He Initialization in TensorFlow:\n",
    "In TensorFlow, He initialization can be implemented using tf.keras.initializers.HeNormal or tf.keras.initializers.HeUniform. Here’s an example using the HeNormal initializer for a simple dense layer:\n",
    "mmary()\r\n",
    "\n",
    "\n",
    "tf.keras.initializers.HeNormal() initializes the weights using He Normal initialization, which is appropriate for ReLU activations.\n",
    "The first layer has 128 neurons with ReLU activation, and the second layer is the output layer with 10 neurons for classification.\n",
    "\n",
    "\n",
    "In this PyTorch code:\n",
    "\n",
    "torch.nn.init.kaiming_normal_() is used to apply He normal initialization to the weights of the layers.\n",
    "The mode='fan_in' parameter specifies that the initialization depends on the number of input units (to ensure proper variance scaling).\n",
    "The nonlinearity='relu' parameter tells the function that we are using ReLU activation, which is important for He initialization.\n",
    "Explanation of He Initialization:\n",
    "He Initialization is specifically designed to maintain the variance of the activations throughout the layers of the network. This helps prevent the vanishing gradients problem, especially when using ReLU or its variants as the activation function.\n",
    "The weights are initialized from a normal distribution with a mean of 0 and a varture\r\n",
    "print(model)\r\n",
    "n\n",
    "fan_in\n",
    "2\n",
    "​\n",
    " , where fan_in is the number of input units in the layer.\n",
    "This ensures that the gradient flow is well-behaved during backpropagation.\n",
    "With these implementations, you can initialize the weights of your neural network using He initialization in either TensorFlow or PyTorch.\n",
    "2. He Initialization in PyTorch:\n",
    "In PyTorch, He initialization can be implemented using the torch.nn.init.kaiming_normal_() or torch.nn.init.kaiming_uniform_() function. Here’s an example using kaiming_normal_() for a simple neural network:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36bb64a0-6b11-4f44-8d15-788f008dbf49",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple model using He initialization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(), input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9ec77f3-94cb-4158-859c-9ec939197e95",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple model using He initialization in PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)  # Input layer with 784 features, 128 neurons\n",
    "        self.fc2 = nn.Linear(128, 10)   # Output layer with 10 neurons for classification\n",
    "\n",
    "        # Apply He initialization to the weights\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation\n",
    "        x = self.fc2(x)          # Output layer (no activation function)\n",
    "        return F.log_softmax(x, dim=1)  # Log softmax for multi-class classification\n",
    "\n",
    "# Create the model instance\n",
    "model = SimpleNN()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda23da-1c1d-4516-a0a2-5e8c7db8c626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
